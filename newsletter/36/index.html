<!DOCTYPE html>

<html lang="de">
<head>
<meta charset="utf-8"/>
<title>Homepage Tobias Fränzel</title>
<meta content="" name="description"/>
<meta content="width=device-width, initial-scale=1" name="viewport">
<link href="../../styles/styles.css" rel="stylesheet" type="text/css"/>
<link href="../../styles/newsletter_styles.css" rel="stylesheet" type="text/css"/>
<link href="../../img/favicon.png" rel="icon" type="image/png"/>
<!-- Mailchimp signup styles -->
<link href="//cdn-images.mailchimp.com/embedcode/classic-10_7.css" rel="stylesheet" type="text/css"/>
<style type="text/css">
      #mc_embed_signup form{padding:0;margin-top:2em;}
    	/* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
    	   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
    </style>
</meta></head>
<body>
<nav class="menu-main">
<ul>
<li><a href="../../index.html">Tobias Fränzel</a></li>
<li><a href="../../newsletter.html">Newsletter</a></li>
<li><a href="../../projekte.html">Projekte</a></li>
<li><a href="../../kontakt.html">Kontakt</a></li>
</ul>
</nav>
<hr class="divider"/>
<div id="container-main">
<h1>KI News #36</h1><span>
                        
                            Hallo und herzlich willkommen zur sechsunddreißigsten Ausgabe von KI News. In dieser Ausgabe mit einer KI für Minecraft, wie man Deepfakes in Videoanrufen erkennt, wofür die FIFA bei der WM 2022 KI einsetzen will und mehr.<p>
Viel Spaß beim Lesen!
                        </p></span><h2 id="2">Minecraft-KI von OpenAI</h2><span><p>
OpenAI hat eine KI für das Spiel <em>Minecraft</em> entwickelt. Dazu haben sie zwei neuronale Netze trainiert, wobei das erste nur dazu dient, die Trainingsdaten für das zweite vorzubereiten.</p><p></p><p>
im ersten Schritt haben die Forscher:innen Leute 2.000 Stunden lang Minecraft spielen lassen und dabei sowohl ihren Bildschirm als auch die Maus- und Tastatur-Eingaben aufgezeichnet.</p><p>
Mit den so gesammelten Daten haben sie ein neuronales Netz darauf trainiert, aus mehreren aufgezeichneten Bildern die Eingaben der Spieler:innen vorherzusagen.</p><p></p><p>
Das so entstandene Modell haben sie dann benutzt, um weitere 70.000 Stunden Minecraft-Videos aus dem Internet mit den passenden Maus- und Tastatur-Daten zu versehen.</p><p>
Auf diesem großen Datensatz haben sie dann ein weiteres Modell trainiert, das die Eingaben der Spieler:innen nur aus den vorhergehenden Bildern vorhersagen soll.</p><p></p><p>
Dieses Modell kann also aus dem, was auf dem Bildschirm zu sehen ist, ableiten, was es als nächstes tun muss - und somit Minecraft spielen.</p><p>
Die Forscher:innen nennen diese Version das "Foundation Modell". Es kann im Spiel relativ einfache Dinge tun, wie zum Beispiel Bäume zu fällen um Stämme zu gewinnen, aus diesen Stämmen Bretter machen und die Bretter wiederum zu einem Tisch zusammen bauen.</p><p></p><p>
Um die Fähigkeiten des Foundation Modells zu verbessern, haben sie nochmal Trainingsdaten in Form von Video-Material und den dazu gehörigen Eingaben gesammelt. Nachdem sie das Modell auch noch damit trainiert hatten, wurde es nochmal besser und konnte zum Beispiel auch Werkzeuge aus Stein herstellen.
<ul>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Veröffentlichung der Forscher:innen: <a href="https://arxiv.org/abs/2206.11795" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://arxiv.org/abs/2206.11795</a></li>
</ul></p></span><h2 id="3">DeepMind KI lernt wie ein Baby</h2><span><p>
Neuronale Netze können erstaunliche Dinge, aber versagen auch immer wieder auf erstaunliche Weise, weil sie kein Verständnis dafür haben, wie die Welt funktioniert. Ihnen fehlt das "Allgemeinwissen" und das, was man als "gesunden Menschenverstand" bezeichnet.</p><p></p><p>
Als einen ersten Schritt in Richtung "Verständnis wie die Welt funktioniert" haben Forscher:innen von Deepmind ein Modell entwickelt, das einfache physikalische Grundlagen gelernt hat.</p><p>
Dabei haben sie sich als Vorbild genommen, wie Babys physikalische Interaktionen zwischen Objekten verstehen.</p><p></p><p>
Ihr Ansatz beruht auf Erkenntnissen aus der Entwicklungspsychologie. Erstens, dass das intuitive Physikverständnis auf einer Reihe von unabhängigen Konzepten basiert, z.B. <a href="https://de.wikipedia.org/wiki/Objektpermanenz" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Objektpermanenz</a> und Kontinuität; und zweitens, dass Menschen aus diesen Konzepten eine Vorstellung davon bilden, was als nächstes passieren wird.</p><p></p><p>
Die Forscher:innen haben mithilfe von Videos von einfachen Objekten, wie Bällen und Würfeln, ein Modell trainiert. Das Modell kann ebenfalls vorhersagen was mit diesen Objekten als nächstes passieren wird, zum Beispiel, wenn sie zusammenstoßen.</p><p>
Zusätzlich kann man auch herausfinden, ob das Modell "überrascht" ist, wenn im Video etwas unerwartetes oder unmögliches passiert, indem man den Unterschied zwischen Vorhersage und Video misst.
<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Artikel bei Nature: <a href="https://www.nature.com/articles/d41586-022-01921-7" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://www.nature.com/articles/d41586-022-01921-7</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Veröffentlichung der Forscher:innen: <a href="https://www.nature.com/articles/s41562-022-01394-8" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://www.nature.com/articles/s41562-022-01394-8</a></li>
</ul></p></span><h2 id="4">Zusammengefasst</h2><span><p></p><p><em><strong><span style="font-size:18px">KI bei der Fußball WM 2022</span></strong></em></p><p>
Die FIFA will ein KI-System bei der umstrittenen Weltmeisterschaft in Katar einsetzen. Dieses basiert auf zwölf Tracking-Kameras im Stadion. Aus den Daten dieser Kameras werden für jeden Spieler auf dem Feld fünzig mal pro Sekunde die Position und 29 weitere Datenpunkte berechnet, zum Beispiel die Positionen der Körperteile, die für Abseitsentscheidungen relevant sind.</p><p>
Außerdem enthält der Ball einen Trägheitssensor, der 500 mal pro Sekunde Daten übermittelt, mit denen sich seine Position präzise berechnen lässt.</p><p>
Das System kann so erkennen, wo der Ball gespielt wurde und wo die Spieler zu diesem Zeitpunkt waren und so Abseitssituationen automatisch erkennen.</p><p>
Aus den Daten wird dann eine 3D-Animation erstellt, so dass die Zuschauer die Entscheidung nachvollziehen können.
<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Mitteilung der FIFA: <a href="https://www.fifa.com/technical/media-releases/semi-automated-offside-technology-to-be-used-at-fifa-world-cup-2022-tm" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://www.fifa.com/technical/media-releases/semi-automated-offside-technology-to-be-used-at-fifa-world-cup-2022-tm</a></li>
</ul><div class="source"><br/>
<em><strong><span style="font-size:18px">So erkennt man Deepfakes in Videoanrufen</span></strong></em><br/>
Forscher:innen der Universität von Kalifornien, Berkeley und der NSA haben ein System entwickelt, mit dem sich Deepfakes in Videoanrufen erkennen lassen.<br/>
Dazu nutzen sie die Tatsache aus, dass vom Gesicht des/der Gesprächspartner:in das Licht des Bildschirms reflektiert wird. Dadurch erscheint es heller oder dunkler, je nachdem was der Bildschirm anzeigt.<br/>
Wenn man den/die andere:n bittet, eine bestimmte Animation abzuspielen, kann man die erwartete Helligkeitsänderung mit der tatsächlichen vergleichen. Da die Beleuchtung durch den Bildschirm bei Deepfakes üblicherweise nicht berücksichtigt wird, kann man die Fakes so erkennen.<br/>
<br/>
Ein Blogeintrag von Metaphysic.ai beschreibt sogar noch eine einfachere Methode: um aktuelle Deepfake-Software zu verwirren, reicht es scheinbar, den/die Anrufer:in zu bitten, den Kopf um 90° zur Seite zu drehen.
<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Artikel bei Unite AI: <a href="https://www.unite.ai/detecting-deepfake-video-calls-through-monitor-illumination/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://www.unite.ai/detecting-deepfake-video-calls-through-monitor-illumination/</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Veröffentlichung der Forscher:innen (pdf): <a href="https://farid.berkeley.edu/downloads/publications/cvpr22a.pdf" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://farid.berkeley.edu/downloads/publications/cvpr22a.pdf</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Blogeintrag bei Metaphysic.ai: <a href="https://metaphysic.ai/to-uncover-a-deepfake-video-call-ask-the-caller-to-turn-sideways/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://metaphysic.ai/to-uncover-a-deepfake-video-call-ask-the-caller-to-turn-sideways/</a></li>
</ul>
</div><div class="source">
<div class="source"><br/>
<em><strong><span style="font-size:18px">AlphaFold veröffentlicht alle Proteinstrukturen</span></strong></em><br/>
<em>AlphaFold</em> ist ein Modell von Deepmind, das aus einer Aminosäurensequenz die Struktur eines Proteins vorhersagen kann. Es wurde letztes Jahr von Deepmind als open-source Software veröffentlicht, zusammen mit Daten zu 350.000 Proteinstrukturen, die in der AlphaFold Datenbank gespeichert sind. Seitdem haben die Forscher:innen weitere Einträge zur Datenbank hinzugefügt, so dass es zuletzt rund eine Million waren.<br/>
Jetzt haben sie die Datenbank auf über 200 Millionen Proteinstrukturen erweitert, was fast alle bekannten Proteine umfasst.
<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Blogeintrag von Deepmind: <a href="https://www.deepmind.com/blog/alphafold-reveals-the-structure-of-the-protein-universe" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://www.deepmind.com/blog/alphafold-reveals-the-structure-of-the-protein-universe</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>AlphaFold Protein Structure Database: <a href="https://alphafold.ebi.ac.uk/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://alphafold.ebi.ac.uk/</a></li>
</ul>
<div class="source"> </div>
<div class="source"><em><strong><span style="font-size:18px">BLOOM: Das größte mehrsprachige open-source Sprachmodell</span></strong></em><br/>
Über tausend Forscher:innen aus siebzig Ländern haben zusammen das bisher größte open-source Sprachmodell veröffentlicht, das mehrere Sprachen unterstützt. Sie nennen es <em>BLOOM</em>, was für "BigScience Large Open-science Open-access Multilingual Language Model" steht.<br/>
BLOOM hat 176 Milliarden Parameter und kann damit 46 menschliche sowie 13 Programmiersprachen verarbeiten.<br/>
Der finale Trainingslauf für das Modell dauerte auf dem französischen <a href="http://www.idris.fr/eng/jean-zay/jean-zay-presentation-eng.html" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank"><em>Jean Zay</em> Supercomputer</a> ganze 117 Tage, was durch eine Förderung im geschätzten Wert von drei Millionen Euro ermöglicht wurde.

<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Blogeintrag von BigScience bei Huggingface: <a href="https://bigscience.huggingface.co/blog/bloom" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://bigscience.huggingface.co/blog/bloom</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Link zum Modell: <a href="https://bigscience.huggingface.co/blog/bloom" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://bigscience.huggingface.co/blog/bloom</a></li>
</ul>
</div>
</div>
</div></p></span><h2 id="5">Außerdem</h2><span><ul>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Die britische Regierung plant, das Datensammeln im Internet für jeden Zweck zu erlauben, ohne Widerspruchsmöglichkeit für Rechteinhaber. Ziel ist, es einfacher zu machen, Trainingsdaten für die KI-Entwicklung zu sammeln: 📖 <a href="https://www.gov.uk/government/consultations/artificial-intelligence-and-ip-copyright-and-patents/outcome/artificial-intelligence-and-intellectual-property-copyright-and-patents-government-response-to-consultation#text-and-data-mining" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Mitteilung der britischen Regierung (ab Absatz 31)</a><br/>
	 </li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Die Metropolitan Police (zuständig für die Greater London Gegend, aber nicht die City of London) will Live-Gesichtserkennung testen, um gesuchte Personen zu finden und Personen, die eine Gefahr für sich selbst oder andere darstellen könnten: 📖 <a href="https://www.met.police.uk/advice/advice-and-information/fr/facial-recognition" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Mitteilung der Metropolitan Police</a><br/>
	 </li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Systeme zum Training von ML Modellen sind 1,8x schneller als letztes Jahr: 📖 <a href="https://spectrum.ieee.org/mlperf-rankings-2022" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Artikel bei IEEE Spectrum</a><br/>
	 </li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Auf Kickstarter kann man eine Katzenklappe vorbestellen, die mit Gesichtserkennung nur die eigene Katze durchlassen soll: 📖 <a href="https://www.kickstarter.com/projects/petvation/petvation-the-smart-automatic-pet-door-powered-with-ai/description" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Projekt auf Kickstarter</a><br/>
	 </li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Neue Version des <em>Practical Deep Learning for Coders</em> Kurses: 📖 <a href="https://www.fast.ai/2022/07/21/dl-coders-22/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Kurs bei fast.ai</a><br/>
	 </li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Forscher:innen der Universität von Chicago haben ein Modell entwickelt, das eine Woche im Voraus die Kriminailität in einem Radius von ca. 300m vorhersagen kann: 📖 <a href="https://www.nature.com/articles/s41562-022-01372-0" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Veröffentlichung bei Nature Human Behavior</a><br/>
	 </li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Forscher:innen des Max-Planck-Instituts für Intelligente Systeme haben einen Roboter-Hund entwickelt, der auf eine ähnliche Weise gehen lernen kann wie Tiere (und Menschen) es tun: 📖 <a href="https://is.mpg.de/de/news/robot-dog-learns-to-walk-in-one-hour" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Mitteilung des MPI</a>, 📖 <a href="https://www.nature.com/articles/s42256-022-00505-4" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Veröffentlichung bei Nature Machine Intelligence</a></li>
</ul></span>
<p id="bottom-nav-container"><a href="../35">« Vorherige</a><a class="" href="../37">Nächste »</a></p>
<!-- Begin Mailchimp Signup Form -->
<div id="mc_embed_signup">
<form action="https://tobiasfraenzel.us7.list-manage.com/subscribe/post?u=6a2f372a93d527ee449b8e785&amp;id=9d690dbb78" class="validate" id="mc-embedded-subscribe-form" method="post" name="mc-embedded-subscribe-form" novalidate="" target="_blank">
<div id="mc_embed_signup_scroll">
<p>Hier abonnieren und keine Ausgabe mehr verpassen:</p>
<div class="indicates-required"><span class="asterisk">*</span> Pflichtfeld</div>
<div class="mc-field-group">
<label for="mce-EMAIL">Email Adresse<span class="asterisk">*</span></label>
<input class="required email" id="mce-EMAIL" name="EMAIL" type="email" value=""/>
</div>
<!--<div class="mc-field-group">
                      <label for="mce-FNAME">Name</label>
                          <input type="text" value="" name="FNAME" class="" id="mce-FNAME">
                  </div>-->
<div class="clear" id="mce-responses">
<div class="response" id="mce-error-response" style="display:none"></div>
<div class="response" id="mce-success-response" style="display:none"></div>
</div> <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
<div aria-hidden="true" style="position: absolute; left: -5000px;"><input name="b_6a2f372a93d527ee449b8e785_9d690dbb78" tabindex="-1" type="text" value=""/></div>
<div class="clear"><input class="button" id="mc-embedded-subscribe" name="subscribe" type="submit" value="Anmelden"/></div>
</div>
</form>
</div>
</div>
</body>
<!-- Matomo -->
<script type="text/javascript">
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
      var u="//tobiasfraenzel.de/misc/piwik/";
      _paq.push(['setTrackerUrl', u+'matomo.php']);
      _paq.push(['setSiteId', '1']);
      var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
      g.type='text/javascript'; g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
    })();
  </script>
<!-- End Matomo Code -->
</html>
