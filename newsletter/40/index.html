<!DOCTYPE html>

<html lang="de">
<head>
<meta charset="utf-8"/>
<title>Homepage Tobias Fränzel</title>
<meta content="" name="description">
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<link href="../../styles/styles.css" rel="stylesheet" type="text/css"/>
<link href="../../styles/newsletter_styles.css" rel="stylesheet" type="text/css"/>
<link href="../../img/favicon.png" rel="icon" type="image/png"/>
<!-- Mailchimp signup styles -->
<link href="//cdn-images.mailchimp.com/embedcode/classic-10_7.css" rel="stylesheet" type="text/css"/>
<style type="text/css">
      #mc_embed_signup form{padding:0;margin-top:2em;}
    	/* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
    	   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
    </style>
</meta></head>
<body>
<nav class="menu-main">
<ul>
<li><a href="../../index.html">Tobias Fränzel</a></li>
<li><a href="../../newsletter.html">Newsletter</a></li>
<li><a href="../../projekte.html">Projekte</a></li>
<li><a href="../../kontakt.html">Kontakt</a></li>
</ul>
</nav>
<hr class="divider"/>
<div id="container-main">
<h1>KI News #40</h1><span>
                        
                            Hallo und herzlich willkommen zur vierzigsten Ausgabe von KI News. Heute dreht sich alles um eines der spannendsten Themen der letzten Monate: Diffusion Models.<p>
Es gab so viele neue Entwicklungen in diesem Bereich, dass ich mich entschieden habe, in dieser Sonderausgabe zu erklären was Diffusion Models sind, wie sie funktionieren und natürlich auch die neuesten Entwicklungen aus der Forschung zu beschreiben.</p><p>
Viel Spaß beim Lesen!
                        </p></span><h2 id="2">Was sind Diffusion Models?</h2><span><p>
Eigentlich handelt es sich dabei nicht um eine neue Art von Modellen, sondern um eine spezielle Methode, um neuronale Netze zu trainieren und zu benutzen. Wie das funktioniert erkläre ich im nächsten Abschnitt.</p><p>
Entwickelt wurden <a href="https://arxiv.org/abs/1503.03585" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Diffusion Models im Jahr 2015</a>, von Forscher:innen in Stanford und der University of California, Berkeley.</p><p></p><p>
Der wichtigste Anwendungsfall für Diffusion Models ist heute das Erzeugen von Bildern. Und tatsächlich wurde das auch schon in der ursprünglichen Veröffentlichung beschrieben.</p><p>
Es dauerte aber noch bis 2021, bis es <a href="https://arxiv.org/abs/2105.05233" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Forscher:innen von OpenAI zum ersten Mal schafften</a>, ein Diffusion Model zu trainieren, das besser war als der bis dahin beste Ansatz zur Bildgenerierung, die sogenannten <a href="https://de.wikipedia.org/wiki/Generative_Adversarial_Networks" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank"><em>GANs</em></a> (<a href="http://tobiasfraenzel.de/newsletter/6/#2" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">mein Newsletter von damals dazu</a>).</p><p></p><p>
Heute gibt es einige Modelle, die auf dem Diffusion-Prinzip basieren und sehr beeindruckende Ergebnisse liefern.</p><p>
Die bekanntesten Beispiele dafür sind wahrscheinlich <a href="https://github.com/CompVis/stable-diffusion" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Stable Diffusion</a> (von <a href="https://ommer-lab.com/research/latent-diffusion-models/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Forscher:innen der LMU in München und der Uni Heidelberg</a>), <a href="https://openai.com/dall-e-2/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">DALL-E</a> (Open AI) und <a href="https://imagen.research.google/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Imagen</a> (Google).
                        </p></span><h2 id="3">Wie funktionieren Diffusion Models?</h2><span><p>
Die Idee für Diffusion Models kommt vom physikalischen Prozess der <a href="https://de.wikipedia.org/wiki/Diffusion" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Diffusion</a>. Dabei verteilt sich ein Stoff, beispielsweise ein Gas oder eine Flüssigkeit, in einem anderen, bis die Konzentration überall gleich ist.</p><p>
Für Diffusion Models wird dieses Konzept auf digitale Daten übertragen, zum Beispiel Bilder.</p><p></p><p><em>Wie funktioniert das?</em></p><p>
Den Bildern aus den Trainingsdaten wird Schritt für Schritt immer mehr zufälliges Rauschen hinzugefügt, bis nur noch Rauschen übrig ist - die Analogie zu einem vollständig verteilten Gas.</p><p>
Mit dieser Reihe von immer stärker verrauschten Bildern wird dann ein neuronales Netz darauf trainiert, das Rauschen, das in einem Schritt hinzugefügt wurde, zu erkennen. Damit kann es dann einen dieser Schritte rückgängig machen.</p><p>
So lernt es, das Rauschen Schritt für Schritt wieder zu entfernen.</p><p></p><p>
Wenn man einem so trainierten neuronalen Netz jetzt kein verrauschtes Bild gibt, sondern einfach zufälliges Rauschen, dann kann man es dazu bringen, ein ganz neues Bild zu generieren.</p><p>
Dazu lässt man es einfach das Rauschen Schritt für Schritt in ein Bild umwandeln.</p><p></p><p>
Durch zusätzliche Eingabedaten kann das Modell außerdem darauf trainiert werden, eine bestimmte Art von Bildern zu erzeugen, die zum Beispiel zu einem Text passen.
                        </p></span><h2 id="4">Aktuelle Entwicklungen bei Diffusion Models</h2><span><div class="source"> </div><em><strong><span style="font-size:18px">Imagen Video</span></strong></em><div class="source">
<div class="source"><em>Imagen Video</em> ist ein Diffusion Modell, das ein Video passend zu einem Text generieren kann. Entwickelt wurde es von Forscher:innen bei Google Research.<br/>
Imagen Video benutzt einen mehrstufigen Prozess: Zuerst wird der Eingabetext durch ein neuronales Netz (ein "Encoder") geschickt, der ihn vorverarbeitet. Danach folgen eine Reihe von Diffusion Modellen. Das erste generiert ein Video mit geringer Auflösung und nur drei Bildern pro Sekunde. Die darauf folgenden Modelle erhöhen Schritt für Schritt sowohl Auflösung als auch Bildrate, bis am Ende ein Video mit einer Auflösung von 1280×768 Pixeln und 24 Bildern pro Sekunde steht.
<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Webseite mit Beispielen: <a href="https://imagen.research.google/video/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://imagen.research.google/video/</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Veröffentlichung der Forscher:innen (pdf): <a href="https://imagen.research.google/video/paper.pdf" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://imagen.research.google/video/paper.pdf</a></li>
</ul>
<div class="source"> </div>
<div class="source"><em><strong><span style="font-size:18px">3DiM: Neue Blickwinkel durch ein Diffusion Model</span></strong></em><br/>
Dass Diffusion Modelle nicht nur Bilder und Videos generieren können, sondern auch mit dreidimensionalen Darstellungen umgehen können, zeigt ein anderes Forscher:innen-Team von Google.<br/>
Das <em>3DiM</em> genannte Modell bekommt als Eingabe ein Bild von einem Gegenstand, seine aktuelle Stellung und eine Ziel-Stellung. Daraus kann es dann ein Bild des Gegenstands in der Ziel-Stellung erzeugen.<br/>
Ein Beispiel: Das Eingabebild ist ein Auto, das von vorne rechts zu sehen ist. In der Ziel-Stellung ist das Auto um 180° gedreht, so dass man es von hinten links sehen würde. Dann kann das Modell ein Bild des Autos von hinten links generieren.
<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Webseite mit Beispielen: <a href="https://3d-diffusion.github.io/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://3d-diffusion.github.io/</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Veröffentlichung der Forscher:innen: <a href="https://arxiv.org/abs/2210.04628" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://arxiv.org/abs/2210.04628</a></li>
</ul>
<div class="source">
<div class="source">
<div class="source"> 
<div class="source"><em><strong><span style="font-size:18px">Human Motion Diffusion Model</span></strong></em><br/>
Das <em>Human Motion Diffusion Model</em> wurde von Forscher:innen der Universität von Tel Aviv entwickelt. Es kann eine Animation von einem sich bewegenden Menschen generieren.<br/>
Welche Art von Bewegung der Mensch machen soll, lässt sich dabei auf verschiedene Weise beeinflussen. Der wichtigste Fall ist ein kurzer Text, der die Bewegung beschreibt, zum Beispiel "Eine Person läuft langsam rückwärts". Es kann aber auch nur eine Art von Bewegung vorgegeben werden, beispielsweise "Rennen". Außerdem kann das Modell nicht nur komplett neue Animationen erzeugen, sondern auch nur einen Teil einer bereits bestehenden Bewegung verändern.<br/>
Die Diffusion Models, die Bilder generieren, basieren üblicherweise auf einem neuronalen Netz, das auf Bilder spezialisiert ist, zum Beispiel einem <a href="https://en.wikipedia.org/wiki/U-Net" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">U-Net</a>. Im Gegensatz dazu basiert das <em>Human Motion Diffusion Model</em> auf einem <a href="https://de.wikipedia.org/wiki/Transformer_(Maschinelles_Lernen)" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Transformer</a>, der in jedem Diffusions-Schritt die Bewegung vorhersagt.

<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Webseite mit Beispielen: <a href="https://guytevet.github.io/mdm-page/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://guytevet.github.io/mdm-page/</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Veröffentlichung der Forscher:innen: <a href="https://arxiv.org/abs/2209.14916" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://arxiv.org/abs/2209.14916</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Code auf Github: <a href="https://github.com/GuyTevet/motion-diffusion-model" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://github.com/GuyTevet/motion-diffusion-model</a></li>
</ul>
<div class="source"><br/>
<em><strong><span style="font-size:18px">DreamFusion</span></strong></em><br/>
Das <em>DreamFusion</em> Modell, von Forscher:innen von Google und der University of California, Berkeley, kann aus einer Beschreibung ein 3D Bild erzeugen. Der Prozess dafür ist allerdings recht komplex.<br/>
Zuerst wird für jede Beschreibung ein sogenanntes <a href="https://www.matthewtancik.com/nerf" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Neural Radiance Field (NeRF)</a> trainiert, eine Art 3D-Darstellung in Form eines neuronalen Netzes.<br/>
Aus dieser Darstellung wird dann ein zufälliger Blickwinkel genommen, Licht und Schatten werden hinzugefügt und am Ende noch zufälliges Rauschen. Dann wird das <em><a href="https://imagen.research.google/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Imagen</a></em> Modell benutzt, um aus dem verrauschten Bild wieder ein gutes Bild zu machen.<br/>
Der Unterschied zwischen dem Bild vor dem Verrauschen und der Ausgabe von Imagen wird dann benutzt, um das NeRF zu trainieren, aus jedem Blickwinkel genauso gute Bilder zu erzeugen wie Imagen.
<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Webseite mit Beispielen: <a href="https://dreamfusion3d.github.io/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://dreamfusion3d.github.io/</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Veröffentlichung der Forscher:innen: <a href="https://arxiv.org/abs/2209.14988" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://arxiv.org/abs/2209.14988</a></li>
</ul>
<br/>
<em><strong><span style="font-size:18px">Re-Imagen</span></strong></em><br/>
Das <em>Re</em> in <em>Re-Imagen</em> steht für <em>Retrieval</em>, also das Abrufen von Daten. Damit beheben die Forscher:innen eine Schwäche der aktuellen Diffusion Models. Diese haben Probleme, gute Bilder zu erzeugen, wenn etwas nur relativ selten in den Trainingsdaten vorkommt.<br/>
Die Lösung dafür ist, den Eingabetext nicht nur an das Modell weiterzugeben, sondern auch zu ihm passende Bilder aus einer Datenbank abzufragen. So kann man für jede Eingabe passende Bilder finden, und sie dem Modell während dem Generieren des neuen Bildes als Zusatzinformationen geben.<br/>
Dadurch konnte besonders die Qualität von Bildern verbessert werden, deren Beschreibungen nur selten in den Trainingsdaten vorkommen.
<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Veröffentlichung der Forscher:innen: <a href="https://arxiv.org/abs/2209.14491" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://arxiv.org/abs/2209.14491</a></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div></span><h2 id="5">Außerdem</h2><span><ul>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Microsoft hat angekündigt, DALL-E 2 in einige seiner Programme einzubinden: 📖 <a href="https://techcrunch.com/2022/10/12/microsoft-brings-dall-e-2-to-the-masses-with-designer-and-image-creator/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Artikel bei Techcrunch</a><br/>
	 </li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Shutterstock hat angekündigt, neben den üblichen Stock-Fotos in Zukunft auch Bilder zu verkaufen, die von DALL-E 2 generiert wurden: 📖 <a href="https://www.theverge.com/2022/10/25/23422359/shutterstock-ai-generated-art-openai-dall-e-partnership-contributors-fund-reimbursement" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Artikel bei The Verge</a><br/>
	 </li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Der Konkurrent Getty Images hält das Verkaufen von KI-generierten Bildern dagegen für gefährlich und möglicherweise illegal: 📖 <a href="https://www.theverge.com/2022/10/25/23422412/getty-images-ai-art-banned-dangerous-bria-partnership" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Artikel bei The Verge</a><br/>
	 </li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Blogeintrag darüber, welchen Einfluss die aktuellen ML Modelle auf Pixelart Künstler haben: 📖 <a href="https://pixelparmesan.com/ai-and-the-future-of-pixel-art/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Blogeintrag bei Pixelparmesan</a><br/>
	 </li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Stability AI, die Firma, die Stable Diffusion vermarktet, hat Version 2 veröffentlicht: 📖 <a href="https://stability.ai/blog/stable-diffusion-v2-release" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Blogeintrag bei Stability AI</a><br/>
	 </li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Kurs von Huggingface zu Diffusion Models: 📖 <a href="https://github.com/huggingface/diffusion-models-class" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Kurs auf Github</a></li>
</ul></span>
<p id="bottom-nav-container"><a href="../39">« Vorherige</a><a class="hidden" href="">Nächste »</a></p>
<!-- Begin Mailchimp Signup Form -->
<div id="mc_embed_signup">
<form action="https://tobiasfraenzel.us7.list-manage.com/subscribe/post?u=6a2f372a93d527ee449b8e785&amp;id=9d690dbb78" class="validate" id="mc-embedded-subscribe-form" method="post" name="mc-embedded-subscribe-form" novalidate="" target="_blank">
<div id="mc_embed_signup_scroll">
<p>Hier abonnieren und keine Ausgabe mehr verpassen:</p>
<!--<div class="indicates-required"><span class="asterisk">*</span> Pflichtfeld</div>-->
<div class="mc-field-group">
<label for="mce-EMAIL">E-Mail Adresse:<!--<span class="asterisk">*</span>--></label>
<input class="required email" id="mce-EMAIL" name="EMAIL" type="email" value=""/>
</div>
<!--<div class="mc-field-group">
                      <label for="mce-FNAME">Name</label>
                          <input type="text" value="" name="FNAME" class="" id="mce-FNAME">
                  </div>-->
<div class="clear" id="mce-responses">
<div class="response" id="mce-error-response" style="display:none"></div>
<div class="response" id="mce-success-response" style="display:none"></div>
</div> <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
<div aria-hidden="true" style="position: absolute; left: -5000px;"><input name="b_6a2f372a93d527ee449b8e785_9d690dbb78" tabindex="-1" type="text" value=""/></div>
<div class="clear"><input class="button" id="mc-embedded-subscribe" name="subscribe" type="submit" value="Anmelden"/></div>
</div>
</form>
</div>
</div>
</body>
<!-- Matomo -->
<script type="text/javascript">
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
      var u="//tobiasfraenzel.de/misc/piwik/";
      _paq.push(['setTrackerUrl', u+'matomo.php']);
      _paq.push(['setSiteId', '1']);
      var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
      g.type='text/javascript'; g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
    })();
  </script>
<!-- End Matomo Code -->
</html>
