<!DOCTYPE html>

<html lang="de">
<head>
<meta charset="utf-8"/>
<title>Homepage Tobias Fr√§nzel</title>
<meta content="" name="description">
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<link href="../../styles/styles.css" rel="stylesheet" type="text/css"/>
<link href="../../styles/newsletter_styles.css" rel="stylesheet" type="text/css"/>
<link href="../../img/favicon.png" rel="icon" type="image/png"/>
<!-- Mailchimp signup styles -->
<link href="//cdn-images.mailchimp.com/embedcode/classic-10_7.css" rel="stylesheet" type="text/css"/>
<style type="text/css">
      #mc_embed_signup form{padding:0;margin-top:2em;}
    	/* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
    	   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
    </style>
</meta></head>
<body>
<nav class="menu-main">
<ul>
<li><a href="../../index.html">Tobias Fr√§nzel</a></li>
<li><a href="../../newsletter.html">Newsletter</a></li>
<li><a href="../../projekte.html">Projekte</a></li>
<li><a href="../../kontakt.html">Kontakt</a></li>
</ul>
</nav>
<hr class="divider"/>
<div id="container-main">
<h1>KI News #40</h1><span>
                        
                            Hallo und herzlich willkommen zur vierzigsten Ausgabe von KI News. Heute dreht sich alles um eines der spannendsten Themen der letzten Monate: Diffusion Models.<p>
Es gab so viele neue Entwicklungen in diesem Bereich, dass ich mich entschieden habe, in dieser Sonderausgabe zu erkl√§ren was Diffusion Models sind, wie sie funktionieren und nat√ºrlich auch die neuesten Entwicklungen aus der Forschung zu beschreiben.</p><p>
Viel Spa√ü beim Lesen!
                        </p></span><h2 id="2">Was sind Diffusion Models?</h2><span><p>
Eigentlich handelt es sich dabei nicht um eine neue Art von Modellen, sondern um eine spezielle Methode, um neuronale Netze zu trainieren und zu benutzen. Wie das funktioniert erkl√§re ich im n√§chsten Abschnitt.</p><p>
Entwickelt wurden <a href="https://arxiv.org/abs/1503.03585" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Diffusion Models im Jahr 2015</a>, von Forscher:innen in Stanford und der University of California, Berkeley.</p><p></p><p>
Der wichtigste Anwendungsfall f√ºr Diffusion Models ist heute das Erzeugen von Bildern. Und tats√§chlich wurde das auch schon in der urspr√ºnglichen Ver√∂ffentlichung beschrieben.</p><p>
Es dauerte aber noch bis 2021, bis es <a href="https://arxiv.org/abs/2105.05233" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Forscher:innen von OpenAI zum ersten Mal schafften</a>, ein Diffusion Model zu trainieren, das besser war als der bis dahin beste Ansatz zur Bildgenerierung, die sogenannten <a href="https://de.wikipedia.org/wiki/Generative_Adversarial_Networks" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank"><em>GANs</em></a> (<a href="http://tobiasfraenzel.de/newsletter/6/#2" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">mein Newsletter von damals dazu</a>).</p><p></p><p>
Heute gibt es einige Modelle, die auf dem Diffusion-Prinzip basieren und sehr beeindruckende Ergebnisse liefern.</p><p>
Die bekanntesten Beispiele daf√ºr sind wahrscheinlich <a href="https://github.com/CompVis/stable-diffusion" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Stable Diffusion</a> (von <a href="https://ommer-lab.com/research/latent-diffusion-models/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Forscher:innen der LMU in M√ºnchen und der Uni Heidelberg</a>), <a href="https://openai.com/dall-e-2/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">DALL-E</a> (Open AI) und <a href="https://imagen.research.google/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Imagen</a> (Google).
                        </p></span><h2 id="3">Wie funktionieren Diffusion Models?</h2><span><p>
Die Idee f√ºr Diffusion Models kommt vom physikalischen Prozess der <a href="https://de.wikipedia.org/wiki/Diffusion" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Diffusion</a>. Dabei verteilt sich ein Stoff, beispielsweise ein Gas oder eine Fl√ºssigkeit, in einem anderen, bis die Konzentration √ºberall gleich ist.</p><p>
F√ºr Diffusion Models wird dieses Konzept auf digitale Daten √ºbertragen, zum Beispiel Bilder.</p><p></p><p><em>Wie funktioniert das?</em></p><p>
Den Bildern aus den Trainingsdaten wird Schritt f√ºr Schritt immer mehr zuf√§lliges Rauschen hinzugef√ºgt, bis nur noch Rauschen √ºbrig ist - die Analogie zu einem vollst√§ndig verteilten Gas.</p><p>
Mit dieser Reihe von immer st√§rker verrauschten Bildern wird dann ein neuronales Netz darauf trainiert, das Rauschen, das in einem Schritt hinzugef√ºgt wurde, zu erkennen. Damit kann es dann einen dieser Schritte r√ºckg√§ngig machen.</p><p>
So lernt es, das Rauschen Schritt f√ºr Schritt wieder zu entfernen.</p><p></p><p>
Wenn man einem so trainierten neuronalen Netz jetzt kein verrauschtes Bild gibt, sondern einfach zuf√§lliges Rauschen, dann kann man es dazu bringen, ein ganz neues Bild zu generieren.</p><p>
Dazu l√§sst man es einfach das Rauschen Schritt f√ºr Schritt in ein Bild umwandeln.</p><p></p><p>
Durch zus√§tzliche Eingabedaten kann das Modell au√üerdem darauf trainiert werden, eine bestimmte Art von Bildern zu erzeugen, die zum Beispiel zu einem Text passen.
                        </p></span><h2 id="4">Aktuelle Entwicklungen bei Diffusion Models</h2><span><div class="source">¬†</div><em><strong><span style="font-size:18px">Imagen Video</span></strong></em><div class="source">
<div class="source"><em>Imagen Video</em> ist ein Diffusion Modell, das ein Video passend zu einem Text generieren kann. Entwickelt wurde es von Forscher:innen bei Google Research.<br/>
Imagen Video benutzt einen mehrstufigen Prozess: Zuerst wird der Eingabetext durch ein neuronales Netz (ein "Encoder") geschickt, der ihn vorverarbeitet. Danach folgen eine Reihe von Diffusion Modellen. Das erste generiert ein Video mit geringer Aufl√∂sung und nur drei Bildern pro Sekunde. Die darauf folgenden Modelle erh√∂hen Schritt f√ºr Schritt sowohl Aufl√∂sung als auch Bildrate, bis am Ende ein Video mit einer Aufl√∂sung von 1280√ó768 Pixeln und 24 Bildern pro Sekunde steht.
<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Webseite mit Beispielen: <a href="https://imagen.research.google/video/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://imagen.research.google/video/</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Ver√∂ffentlichung der Forscher:innen (pdf): <a href="https://imagen.research.google/video/paper.pdf" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://imagen.research.google/video/paper.pdf</a></li>
</ul>
<div class="source">¬†</div>
<div class="source"><em><strong><span style="font-size:18px">3DiM: Neue Blickwinkel durch ein Diffusion Model</span></strong></em><br/>
Dass Diffusion Modelle nicht nur Bilder und Videos generieren k√∂nnen, sondern auch mit dreidimensionalen Darstellungen umgehen k√∂nnen, zeigt ein anderes Forscher:innen-Team von Google.<br/>
Das <em>3DiM</em> genannte Modell bekommt als Eingabe ein Bild von einem Gegenstand, seine aktuelle Stellung und eine Ziel-Stellung. Daraus kann es dann ein Bild des Gegenstands in der Ziel-Stellung erzeugen.<br/>
Ein Beispiel: Das Eingabebild ist ein Auto, das von vorne rechts zu sehen ist. In der Ziel-Stellung ist das Auto um 180¬∞ gedreht, so dass man es von hinten links sehen w√ºrde. Dann kann das Modell ein Bild des Autos von hinten links generieren.
<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Webseite mit Beispielen: <a href="https://3d-diffusion.github.io/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://3d-diffusion.github.io/</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Ver√∂ffentlichung der Forscher:innen: <a href="https://arxiv.org/abs/2210.04628" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://arxiv.org/abs/2210.04628</a></li>
</ul>
<div class="source">
<div class="source">
<div class="source">¬†
<div class="source"><em><strong><span style="font-size:18px">Human Motion Diffusion Model</span></strong></em><br/>
Das <em>Human Motion Diffusion Model</em> wurde von Forscher:innen der Universit√§t von Tel Aviv entwickelt. Es kann eine Animation von einem sich bewegenden Menschen generieren.<br/>
Welche Art von Bewegung der Mensch machen soll, l√§sst sich dabei auf verschiedene Weise beeinflussen. Der wichtigste Fall ist ein kurzer Text, der die Bewegung beschreibt, zum Beispiel "Eine Person l√§uft langsam r√ºckw√§rts". Es kann aber auch nur eine Art von Bewegung vorgegeben werden, beispielsweise "Rennen". Au√üerdem kann das Modell nicht nur komplett neue Animationen erzeugen, sondern auch nur einen Teil einer bereits bestehenden Bewegung ver√§ndern.<br/>
Die Diffusion Models, die Bilder generieren, basieren √ºblicherweise auf einem neuronalen Netz, das auf Bilder spezialisiert ist, zum Beispiel einem <a href="https://en.wikipedia.org/wiki/U-Net" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">U-Net</a>. Im Gegensatz dazu basiert das <em>Human Motion Diffusion Model</em> auf einem <a href="https://de.wikipedia.org/wiki/Transformer_(Maschinelles_Lernen)" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Transformer</a>, der in jedem Diffusions-Schritt die Bewegung vorhersagt.

<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Webseite mit Beispielen: <a href="https://guytevet.github.io/mdm-page/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://guytevet.github.io/mdm-page/</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Ver√∂ffentlichung der Forscher:innen: <a href="https://arxiv.org/abs/2209.14916" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://arxiv.org/abs/2209.14916</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Code auf Github: <a href="https://github.com/GuyTevet/motion-diffusion-model" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://github.com/GuyTevet/motion-diffusion-model</a></li>
</ul>
<div class="source"><br/>
<em><strong><span style="font-size:18px">DreamFusion</span></strong></em><br/>
Das <em>DreamFusion</em> Modell, von Forscher:innen von Google und der University of California, Berkeley, kann aus einer Beschreibung ein 3D Bild erzeugen. Der Prozess daf√ºr ist allerdings recht komplex.<br/>
Zuerst wird f√ºr jede Beschreibung ein sogenanntes <a href="https://www.matthewtancik.com/nerf" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Neural Radiance Field (NeRF)</a> trainiert, eine Art 3D-Darstellung in Form eines neuronalen Netzes.<br/>
Aus dieser Darstellung wird dann ein zuf√§lliger Blickwinkel genommen, Licht und Schatten werden hinzugef√ºgt und am Ende noch zuf√§lliges Rauschen. Dann wird das <em><a href="https://imagen.research.google/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Imagen</a></em> Modell benutzt, um aus dem verrauschten Bild wieder ein gutes Bild zu machen.<br/>
Der Unterschied zwischen dem Bild vor dem Verrauschen und der Ausgabe von Imagen wird dann benutzt, um das NeRF zu trainieren, aus jedem Blickwinkel genauso gute Bilder zu erzeugen wie Imagen.
<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Webseite mit Beispielen: <a href="https://dreamfusion3d.github.io/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://dreamfusion3d.github.io/</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Ver√∂ffentlichung der Forscher:innen: <a href="https://arxiv.org/abs/2209.14988" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://arxiv.org/abs/2209.14988</a></li>
</ul>
<br/>
<em><strong><span style="font-size:18px">Re-Imagen</span></strong></em><br/>
Das <em>Re</em> in <em>Re-Imagen</em> steht f√ºr <em>Retrieval</em>, also das Abrufen von Daten. Damit beheben die Forscher:innen eine Schw√§che der aktuellen Diffusion Models. Diese haben Probleme, gute Bilder zu erzeugen, wenn etwas nur relativ selten in den Trainingsdaten vorkommt.<br/>
Die L√∂sung daf√ºr ist, den Eingabetext nicht nur an das Modell weiterzugeben, sondern auch zu ihm passende Bilder aus einer Datenbank abzufragen. So kann man f√ºr jede Eingabe passende Bilder finden, und sie dem Modell w√§hrend dem Generieren des neuen Bildes als Zusatzinformationen geben.<br/>
Dadurch konnte besonders die Qualit√§t von Bildern verbessert werden, deren Beschreibungen nur selten in den Trainingsdaten vorkommen.
<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Ver√∂ffentlichung der Forscher:innen: <a href="https://arxiv.org/abs/2209.14491" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://arxiv.org/abs/2209.14491</a></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div></span><h2 id="5">Au√üerdem</h2><span><ul>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Microsoft hat angek√ºndigt, DALL-E 2 in einige seiner Programme einzubinden: üìñ <a href="https://techcrunch.com/2022/10/12/microsoft-brings-dall-e-2-to-the-masses-with-designer-and-image-creator/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Artikel bei Techcrunch</a><br/>
	¬†</li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Shutterstock hat angek√ºndigt, neben den √ºblichen Stock-Fotos in Zukunft auch Bilder zu verkaufen, die von DALL-E 2 generiert wurden: üìñ <a href="https://www.theverge.com/2022/10/25/23422359/shutterstock-ai-generated-art-openai-dall-e-partnership-contributors-fund-reimbursement" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Artikel bei The Verge</a><br/>
	¬†</li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Der Konkurrent Getty Images h√§lt das Verkaufen von KI-generierten Bildern dagegen f√ºr gef√§hrlich und m√∂glicherweise illegal: üìñ <a href="https://www.theverge.com/2022/10/25/23422412/getty-images-ai-art-banned-dangerous-bria-partnership" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Artikel bei The Verge</a><br/>
	¬†</li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Blogeintrag dar√ºber, welchen Einfluss die aktuellen ML Modelle auf Pixelart K√ºnstler haben:¬†üìñ <a href="https://pixelparmesan.com/ai-and-the-future-of-pixel-art/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Blogeintrag bei Pixelparmesan</a><br/>
	¬†</li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Stability AI, die Firma, die Stable Diffusion vermarktet, hat Version 2 ver√∂ffentlicht:¬†üìñ <a href="https://stability.ai/blog/stable-diffusion-v2-release" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Blogeintrag bei Stability AI</a><br/>
	¬†</li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Kurs von Huggingface zu Diffusion Models:¬†üìñ <a href="https://github.com/huggingface/diffusion-models-class" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Kurs auf Github</a></li>
</ul></span>
<p id="bottom-nav-container"><a href="../39">¬´ Vorherige</a><a class="hidden" href="">N√§chste ¬ª</a></p>
<!-- Begin Mailchimp Signup Form -->
<div id="mc_embed_signup">
<form action="https://tobiasfraenzel.us7.list-manage.com/subscribe/post?u=6a2f372a93d527ee449b8e785&amp;id=9d690dbb78" class="validate" id="mc-embedded-subscribe-form" method="post" name="mc-embedded-subscribe-form" novalidate="" target="_blank">
<div id="mc_embed_signup_scroll">
<p>Hier abonnieren und keine Ausgabe mehr verpassen:</p>
<!--<div class="indicates-required"><span class="asterisk">*</span> Pflichtfeld</div>-->
<div class="mc-field-group">
<label for="mce-EMAIL">E-Mail Adresse:<!--<span class="asterisk">*</span>--></label>
<input class="required email" id="mce-EMAIL" name="EMAIL" type="email" value=""/>
</div>
<!--<div class="mc-field-group">
                      <label for="mce-FNAME">Name</label>
                          <input type="text" value="" name="FNAME" class="" id="mce-FNAME">
                  </div>-->
<div class="clear" id="mce-responses">
<div class="response" id="mce-error-response" style="display:none"></div>
<div class="response" id="mce-success-response" style="display:none"></div>
</div> <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
<div aria-hidden="true" style="position: absolute; left: -5000px;"><input name="b_6a2f372a93d527ee449b8e785_9d690dbb78" tabindex="-1" type="text" value=""/></div>
<div class="clear"><input class="button" id="mc-embedded-subscribe" name="subscribe" type="submit" value="Anmelden"/></div>
</div>
</form>
</div>
</div>
</body>
<!-- Matomo -->
<script type="text/javascript">
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
      var u="//tobiasfraenzel.de/misc/piwik/";
      _paq.push(['setTrackerUrl', u+'matomo.php']);
      _paq.push(['setSiteId', '1']);
      var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
      g.type='text/javascript'; g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
    })();
  </script>
<!-- End Matomo Code -->
</html>
