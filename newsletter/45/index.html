<!DOCTYPE html>

<html lang="de">
<head>
<meta charset="utf-8"/>
<title>Homepage Tobias FrÃ¤nzel</title>
<meta content="" name="description"/>
<meta content="width=device-width, initial-scale=1" name="viewport">
<link href="../../styles/styles.css" rel="stylesheet" type="text/css"/>
<link href="../../styles/newsletter_styles.css" rel="stylesheet" type="text/css"/>
<link href="../../img/favicon.png" rel="icon" type="image/png"/>
<!-- Mailchimp signup styles -->
<link href="//cdn-images.mailchimp.com/embedcode/classic-10_7.css" rel="stylesheet" type="text/css"/>
<style type="text/css">
      #mc_embed_signup form{padding:0;margin-top:2em;}
    	/* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
    	   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
    </style>
</meta></head>
<body>
<nav class="menu-main">
<ul>
<li><a href="../../index.html">Tobias FrÃ¤nzel</a></li>
<li><a href="../../newsletter.html">Newsletter</a></li>
<li><a href="../../projekte.html">Projekte</a></li>
<li><a href="../../kontakt.html">Kontakt</a></li>
</ul>
</nav>
<hr class="divider"/>
<div id="container-main">
<h1>KI News #45</h1><span>
                        
                            Hallo und herzlich willkommen zur fÃ¼nfundvierzigsten Ausgabe von KI News. Diesmal gibt es viele neue Modelle, die etwas aus einem Text erzeugen kÃ¶nnen - gesprochene Sprache, Bilder, Musik und 3D-Darstellungen. AuÃŸerdem habe ich ein Update zu den neuesten Entwicklungen bei ChatGPT und noch mehr.<p>
Viel SpaÃŸ beim Lesen!
                        </p></span><h2 id="2">Inhalt</h2><span><ul>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;"><a href="#3">VALL-E TTS: ein neues Text-to-Speech Modell</a></li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;"><a href="#4">Muse: ein Text-to-Image Transformer</a></li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;"><a href="#5">MusicLM kann aus einer Beschreibung Musik machen</a></li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;"><a href="#6">ChatGPT Update</a></li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;"><a href="#7">Zusammengefasst</a>
<ul>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Dream3D: 3D-Darstellungen aus Texten</li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Forward-Forward Algorithmus</li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Stable Attribution: Welche Bilder benutzt Stable DIffusion?</li>
</ul>
</li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;"><a href="#7">AuÃŸerdem</a></li>
</ul></span><h2 id="3">Vall-E TTS: ein neues Text-to-Speech Modell</h2><span><p>
Vall-E TTS ist ein Modell, das Forscher:innen von Microsoft entwickelt haben. TTS steht dabei fÃ¼r Text-to-Speech und beschreibt was das Modell macht: Text in Sprache umwandeln.</p><p>
Trainiert wurde Vall-E mit 60.000 Stunden englischer Sprache von 7.000 verschiedenen Sprecher:innen. Laut den Forscher:innen sind das sehr viel mehr Trainingsdaten als bei bisherigen Modellen, die mit hÃ¶chstens 600 Stunden Sprache trainiert wurden.</p><p></p><p><em>Wie funktioniert Vall-E TTS?</em></p><p>
Als Eingabedaten bekommt es den Text und eine drei Sekunden lange Sprachaufnahme von der Stimme, mit der der Text gesprochen werden soll.</p><p>
Sowohl der Text als auch die Aufnahme werden vorverarbeitet, bevor sie an das Modell weitergegeben werden.</p><p>
Der Text wird in <a href="https://de.wikipedia.org/wiki/Phonem" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Phoneme</a> (Laute mit unterschiedlicher Bedeutung) umgewandelt. Die Sprachaufnahme wird durch einen Audio <a href="https://de.wikipedia.org/wiki/Codec" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Codec</a> in Codes umgewandelt. Als Codec benutzen die Forscher:innen hier das <a href="http://tobiasfraenzel.de/newsletter/42/#4" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">EnCodec Modell</a>.</p><p></p><p>
Die Phoneme und Codes werden dann an ein Sprachmodell weitergegeben. Dieses Modell macht eine Vorhersage, welche Codes in der Aufnahme als nÃ¤chstes kommen wÃ¼rden, unter BerÃ¼cksichtigung des Textes.</p><p>
Diese vorhergesagten Codes werden zum Schluss von einem Decoder wieder in hÃ¶rbare Audiodaten umgewandelt.</p><p></p><p>
Eine Besonderheit von Vall-E TTS ist, dass es auch die Emotionen aus der Sprachaufnahme in die Vorhersage Ã¼bernehmen kann, auch ohne dafÃ¼r speziell trainiert zu sein.
<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Demo-Webseite: <a href="https://valle-demo.github.io/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://valle-demo.github.io/</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>VerÃ¶ffentlichung der Forscher:innen: <a href="https://arxiv.org/abs/2301.02111" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://arxiv.org/abs/2301.02111</a></li>
</ul></p></span><h2 id="4">Muse: ein Text-to-Image Transformer</h2><span><p>
Forscher:innen von Google haben ein Modell namens Muse entwickelt, das zu Beschreibungen passende Bilder generieren kann.</p><p></p><p>
Der Aufbau von Muse ist relativ komplex. Es besteht aus einer ganzen Reihe von Modellen, die jeweils spezielle Aufgaben Ã¼bernehmen.</p><p>
Muse kann als Eingabedaten Texte und Bilder verarbeiten. Diese werden als erstes vorverarbeitet: die Texte von einem Sprachmodell (<a href="https://arxiv.org/abs/1910.10683" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">T5 Text Encoder</a>), zu sogenannten Embeddings, die Bilder von einem <a href="https://compvis.github.io/taming-transformers/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">VQGAN CNN Modell</a>, zu sogenannten Tokens.</p><p></p><p>
Darauf folgen dann zwei aufeinander aufbauende Transformer Modelle:</p><p>
Zuerst das "Base Model". Es lernt, die Tokens eines 256 x 256 Pixel groÃŸen Bildes passend zu den Text Embeddings vorherzusagen.</p><p>
Danach kommt das "Super Resolution Model". Dieses bekommt zusÃ¤tzlich zu den Embeddings noch die Tokens aus dem Base Model und macht damit eine Vorhersage der Tokens eines 512 x 512 Pixel groÃŸen Bildes.</p><p></p><p>
Trainiert wurde Muse auf einem Datensatz aus 460 Millionen Text-Bild-Paaren. Das Training auf 512 spezialisierten Prozessoren (<a href="https://de.wikipedia.org/wiki/Tensor_Processing_Unit" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">TPUs</a>) dauerte eine Woche.

<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Webseite mit Beispielen: <a href="https://muse-model.github.io/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://muse-model.github.io/</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>VerÃ¶ffentlichung der Forscher:innen: <a href="https://arxiv.org/abs/2301.00704" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://arxiv.org/abs/2301.00704</a></li>
</ul></p></span><h2 id="5">MusicLM kann aus einer Beschreibung Musik machen</h2><span><p>
Wie der Name schon andeutet, kann das MusicLM Modell (bis zu fÃ¼nf Minuten lange) MusikstÃ¼cke generieren.</p><p>
Alles was es dazu braucht ist eine Beschreibung, was zu hÃ¶ren sein soll, zum BeispielÂ â€œ<em>enchanting jazz song with a memorable saxophone solo and a solo singer</em>â€.</p><p>
Das von Google entwickelte Modell kann nicht nur vÃ¶llig neue Lieder erzeugen, sondern auch gepfiffene oder gesummte Melodien in andere Musik umwandeln, die von einem Text beschrieben wird.</p><p></p><p>
Beim Training des Modells hatten die Forscher:innen das Problem, dass es nicht besonders viele Beschreibungen von Liedern in der benÃ¶tigten Form gibt.</p><p>
Deshalb haben sie MusicLM nicht direkt auf Text-Musik-Datenpaaren trainiert, sondern einen anderen Weg gewÃ¤hlt.</p><p>
Es gibt ein Modell namens "<a href="https://arxiv.org/abs/2208.12415" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">MuLan</a>", das darauf trainiert ist, Musik und ihre Beschreibung intern sehr Ã¤hnlich darzustellen (eine sogenannte "<a href="https://de.wikipedia.org/wiki/Worteinbettung" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Einbettung</a>", englisch "Embedding"). Dadurch lassen sich die Text-Embeddings direkt aus der Musik ableiten.</p><p>
Das wiederum hat den Vorteil, dass man nicht mehr unbedingt tatsÃ¤chlich Texte braucht, sondern stattdessen die abgeleiteten Embeddings von MuLan benutzen kann.</p><p>
Dadurch konnten die Forscher:innen MusicLM auf reinen Audio-Daten und den daraus berechneten Text-Embeddings trainieren.</p><p></p><p>
MusicLM ist also tatsÃ¤chlich nicht darauf trainiert, Musik aus Texten zu erzeugen, sondern aus den entsprechenden Text-Embeddings von MuLan.</p><p>
Deshalb muss fÃ¼r eine Vorhersage, also das Generieren von Musik, der Text zuerst in ein Embedding umgewandelt werden, bevor er an MusicLM weitergegeben wird.</p><p></p><p>
Um zu prÃ¼fen wie gut das Modell funktioniert, haben die Forscher:innen einen neuen Datensatz erstellt, fÃ¼r den sie, mithilfe von Musikern, 5.500 Musik-Textbeschreibung-Paare gesammelt haben.</p><p>
Diesen haben sie auch verÃ¶ffentlicht, so dass er fÃ¼r die weitere Forschung benutzt werden kann.</p><p></p><p>
Die Evaluierung von fÃ¼nf verschiedenen Metriken hat ergeben, dass MusicLM besser funktioniert als bisherige Modelle wie <a href="http://tobiasfraenzel.de/newsletter/41/#7" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Riffusion</a> und <a href="https://mubert.com/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Mubert</a>.

<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Webseite mit Beispielen: <a href="https://google-research.github.io/seanet/musiclm/examples/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://google-research.github.io/seanet/musiclm/examples/</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>VerÃ¶ffentlichung der Forscher:innen: <a href="https://arxiv.org/abs/2301.11325" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://arxiv.org/abs/2301.11325</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Artikel bei Techcrunch: <a href="https://techcrunch.com/2023/01/27/google-created-an-ai-that-can-generate-music-from-text-descriptions-but-wont-release-it/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://techcrunch.com/2023/01/27/google-created-an-ai-that-can-generate-music-from-text-descriptions-but-wont-release-it/</a></li>
</ul></p></span><h2 id="6">ChatGPT Update</h2><span><ul>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">OpenAI hat APIs fÃ¼r <a href="https://openai.com/blog/chatgpt/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">ChatGPT</a> und <a href="https://openai.com/research/whisper" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Whisper</a> (ein Spracherkennungsmodell) eingefÃ¼hrt. Der Blogeintrag hat auch Beispiele, wie die APIs bereits in Apps genutzt werden (u.a. von Snapchat, Instacart und Shopify): ğŸ“– <a href="https://openai.com/blog/introducing-chatgpt-and-whisper-apis" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">openai.com</a></li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Wie der Hype um ChatGPT die geplante KI-Regulierung der EU, den sogenannten "AI Act", beeinflusst:ğŸ“– <a href="https://www.politico.eu/article/eu-plan-regulate-chatgpt-openai-artificial-intelligence-act/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">politico.eu</a></li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">MIT Technology Review hat mit Mitarbeitenden von OpenAI Ã¼ber ChatGPT gesprochen und gibt einen interessanten Einblick: ğŸ“– <a href="https://www.technologyreview.com/2023/03/03/1069311/inside-story-oral-history-how-chatgpt-built-openai/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">technologyreview.com</a></li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Ein Artikel mit einer Auflistung der Unternehmen, die versuchen, mit ChatGPT zu konkurrieren: ğŸ“– <a href="https://www.theverge.com/2023/3/5/23599209/companies-keep-up-chatgpt-ai-chatbots" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">theverge.com</a></li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Microsoft hat angefangen, die EinschrÃ¤nkungen fÃ¼r den Bing Chat wieder zu lockern: ğŸ“–<a href="https://www.theverge.com/2023/3/8/23631065/microsoft-is-letting-bing-chat-for-longer" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">theverge.com</a></li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Ein Artikel Ã¼ber den aktuellen KI-Hype und die damit einhergehenden Probleme verÃ¶ffentlicht: ğŸ“– <a href="https://www.tagesschau.de/wirtschaft/technologie/kuenstliche-intelligenz-konzerne-plaene-101.html" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">tagesschau.de</a></li>
</ul></span><h2 id="7">Zusammengefasst</h2><span><div class="source">Â </div><em><strong><span style="font-size:18px">Dream3D: 3D-Darstellungen aus Texten</span></strong></em><div class="source">
<div class="source">Forscher:innen aus China haben ein Modell namens Dream3D entwickelt, das eine Beschreibung in eine 3D-Darstellung umwandeln kann.<br/>
Dazu benutzt es zuerst ein anderes Modell, um aus einer Beschreibung ein Bild zu generieren, z.B. <a href="https://de.wikipedia.org/wiki/Stable_Diffusion" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Stable Diffusion</a>.<br/>
Dieses Bild enthÃ¤lt noch keine Details, sondern nur die grundsÃ¤tzliche Form des gewÃ¼nschten Gegenstands.<br/>
Daraus generiert es dann in einem weiteren Schritt eine einfache 3D-Darstellung.<br/>
Diese wird dann wiederum als Eingabe fÃ¼r ein weiteres Modell (<a href="https://arxiv.org/abs/2003.08934" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">NeRF</a>) genutzt, das die fehlenden Details hinzufÃ¼gt.

<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Webseite mit Beispielen: <a href="https://bluestyle97.github.io/dream3d/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://bluestyle97.github.io/dream3d/</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>VerÃ¶ffentlichung der Forscher:innen: <a href="https://arxiv.org/abs/2212.14704" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://arxiv.org/abs/2212.14704</a></li>
</ul>
<div class="source">Â </div>
<div class="source"><strong><em><span style="font-size:18px">Forward-Forward Algorithmus</span></em></strong><br/>
Das Training von neuronalen Netzen funktioniert aktuell grob gesagt so:<br/>
Das neuronale Netz bekommt Eingabedaten und verarbeitet diese, um eine Vorhersage zu machen. Die Vorhersage wird mit dem erwarteten Wert vergleichen. Aus diesem Vergleich wird dann berechnet, welche Ã„nderungen im neuronalen Netz gemacht werden mÃ¼ssen, um die nÃ¤chste Vorhersage nÃ¤her an den erwarteten Wert zu bringen. Zum Schluss werden diese Ã„nderungen dann angewandt.<br/>
Der erste Teil dieses Ablaufs, in dem die Vorhersage gemacht wird, heiÃŸt "Forward Pass", weil er von vorne (Eingabe) nach hinten (Vorhersage) durch das Netz berechnet wird.<br/>
Der zweite Teil, wo das Netz angepasst wird, heiÃŸt "Backpropagation", weil die Anpassungen von hinten nach vorne gemacht werden.<br/>
Der Forscher Geoffrey Hinton von Google Brain hat eine neue Methode verÃ¶ffentlicht, wie die Backpropagation durch einen zweiten Forward Pass ersetzt werden kann, was in manchen FÃ¤llen Vorteile haben kann.
<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Artikel bei InfoQ: <a href="https://www.infoq.com/news/2023/01/hinton-forward-algorithm/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://www.infoq.com/news/2023/01/hinton-forward-algorithm/</a></li>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>VerÃ¶ffentlichung (PDF): <a href="https://www.cs.toronto.edu/~hinton/FFA13.pdf" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://www.cs.toronto.edu/~hinton/FFA13.pdf</a></li>
</ul>
<div class="source">Â </div>
<div class="source">
<div class="source"><strong><em><span style="font-size:18px">Stable Attribution: welche Bilder benutzt Stable Diffusion?</span></em></strong><br/>
Bei Modellen wie <a href="https://de.wikipedia.org/wiki/Stable_Diffusion" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Stable Diffusion</a>, die Bilder generieren kÃ¶nnen, kann man bei der Benutzung nicht mehr sehen, durch welche Bilder in den Trainingsdaten das Modell gelernt hat, ein bestimmtes Bild zu generieren.<br/>
Die Webseite Stable Attribution will das Ã¤ndern. Man kann dort ein Bild hochladen und Stable Attribution sucht dann die Bilder aus den Trainingsdaten von Stable Diffusion heraus, die am "Ã¤hnlichsten" dazu sind. Wie genau die Ã„hnlichkeit berechnet wird, bleibt leider unklar.<br/>
Witzigerweise funktioniert das auch mit normalen Fotos, die nicht KI-generiert sind. Dadurch kann man die Trainingsdaten einfach nach Bildern durchsuchen, die Ã¤hnlich zu einem bestimmten anderen Bild sind. Ich habe das z.B. mit einem Foto von mir ausprobiert und als Ergebnis lauter Bilder von MÃ¤nnern bekommen, die durchaus eine gewisse Ã„hnlichkeit mit mir hatten.
<ul>
<li class="source" style='mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;list-style: "\1F4D6  ";'>Webseite: <a href="https://www.stableattribution.com/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">https://www.stableattribution.com/</a></li>
</ul>
</div>
</div>
</div>
</div>
</div></span><h2 id="8">AuÃŸerdem</h2><span><ul>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Warum Alexa nicht darauf reagiert, wenn jemand in einer Fernsehwerbung "Alexa" sagt: ğŸ“– <a href="https://www.amazon.science/blog/why-alexa-wont-wake-up-when-she-hears-her-name-in-amazons-super-bowl-ad" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Blogeintrag von Amazon</a><br/>
	Â </li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Sehr gute ErklÃ¤rung von KI bei Quarks Dimension Ralph: ğŸ“– <a href="https://www.youtube.com/watch?v=SXvI01KtRKQ" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Video auf Youtube</a><br/>
	Â </li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Der "Scene Optimizer" in der Foto App von Samsung scheint eine Art von KI zu benutzen, damit Fotos detaillierter aussehen als sie tatsÃ¤chlich sind: ğŸ“– <a href="https://www.reddit.com/r/Android/comments/11nzrb0/samsung_space_zoom_moon_shots_are_fake_and_here/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Post auf Reddit</a><br/>
	Â </li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Infinite AI Array: ein witziges Python Package, das mithilfe von GPT-3 dafÃ¼r sorgt, dass einem die Werte in einer Liste nicht ausgehen: ğŸ“– <a href="https://github.com/ianb/infinite-ai-array" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Infinite AI Array auf Github</a><br/>
	Â </li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Jemand hat sich einen Katzen-Detektor programmiert, der ihn benachrichtigt, wenn eine Katze in seinem Garten ist: ğŸ“– <a href="https://blog.aawadia.dev/2022/12/20/cats-pi-and-machine-learning/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Blogeintrag bei aawadia.dev</a><br/>
	Â </li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Deep Learning Tuning Playbook von Forscher:innen von Google und Harvard: ğŸ“– <a href="https://github.com/google-research/tuning_playbook" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Playbook auf Github</a><br/>
	Â </li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Getty Images verklagt Stability AI:Â ğŸ“– <a href="https://copyrightlately.com/pdfviewer/getty-images-v-stability-ai-complaint/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Dokument bei copyrightlately.com</a><br/>
	Â </li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Der Github CEO findet, dass open source Entwickler von der geplanten EU-Regulierung (AI Act) ausgenommen werden sollen:Â ğŸ“– <a href="https://techcrunch.com/2023/02/03/github-ceo-on-why-open-source-developers-should-be-exempt-from-the-eus-ai-act/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Artikel bei Techcrunch</a><br/>
	Â </li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Jemand hat eine iPhone-App programmiert, die einen mithilfe von <a href="https://openai.com/research/clip" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">CLIP</a> die eigenen Fotos durchsuchen lÃ¤sst:Â ğŸ“– <a href="https://mazzzystar.github.io/2022/12/29/Run-CLIP-on-iPhone-to-Search-Photos/" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Beschreibung bei Github Pages</a><br/>
	Â </li>
<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;">Die Rockband Limp Bizkit benutzt Deepfakes in einem Musikvideo, um unter anderem Putin, Selensky und Kim Jong Un auftreten zu lassen:Â ğŸ“– <a href="https://www.youtube.com/watch?v=EnhvI9SCPfk" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;" target="_blank">Video auf Youtube</a></li>
</ul></span>
<p id="bottom-nav-container"><a href="../44">Â« Vorherige</a><a class="" href="../46">NÃ¤chste Â»</a></p>
<!-- Begin Mailchimp Signup Form -->
<div id="mc_embed_signup">
<form action="https://tobiasfraenzel.us7.list-manage.com/subscribe/post?u=6a2f372a93d527ee449b8e785&amp;id=9d690dbb78" class="validate" id="mc-embedded-subscribe-form" method="post" name="mc-embedded-subscribe-form" novalidate="" target="_blank">
<div id="mc_embed_signup_scroll">
<p>Hier abonnieren und keine Ausgabe mehr verpassen:</p>
<!--<div class="indicates-required"><span class="asterisk">*</span> Pflichtfeld</div>-->
<div class="mc-field-group">
<label for="mce-EMAIL">E-Mail Adresse:<!--<span class="asterisk">*</span>--></label>
<input class="required email" id="mce-EMAIL" name="EMAIL" type="email" value=""/>
</div>
<!--<div class="mc-field-group">
                      <label for="mce-FNAME">Name</label>
                          <input type="text" value="" name="FNAME" class="" id="mce-FNAME">
                  </div>-->
<div class="clear" id="mce-responses">
<div class="response" id="mce-error-response" style="display:none"></div>
<div class="response" id="mce-success-response" style="display:none"></div>
</div> <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
<div aria-hidden="true" style="position: absolute; left: -5000px;"><input name="b_6a2f372a93d527ee449b8e785_9d690dbb78" tabindex="-1" type="text" value=""/></div>
<div class="clear"><input class="button" id="mc-embedded-subscribe" name="subscribe" type="submit" value="Anmelden"/></div>
</div>
</form>
</div>
</div>
</body>
<!-- Matomo -->
<script type="text/javascript">
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
      var u="//tobiasfraenzel.de/misc/piwik/";
      _paq.push(['setTrackerUrl', u+'matomo.php']);
      _paq.push(['setSiteId', '1']);
      var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
      g.type='text/javascript'; g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
    })();
  </script>
<!-- End Matomo Code -->
</html>
